{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sports-recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Rw5Qs6-cHM",
        "colab_type": "text"
      },
      "source": [
        "**RICORDARSI DI ANDARE AD edit-> notebook settings-> hardware accellerator GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nQ9dW11IzeNI"
      },
      "source": [
        "## Clone dei repository necessari"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnAPGvmHX2Ex",
        "colab_type": "code",
        "outputId": "11addc01-6cc7-4c81-d40d-8f4c225078cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "! git clone https://github.com/Meyke/ProgettoSIIML.git\n",
        "! git clone --recursive https://github.com/metalbubble/TRN-pytorch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ProgettoSIIML'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 897 (delta 5), reused 8 (delta 4), pack-reused 883\u001b[K\n",
            "Receiving objects: 100% (897/897), 141.53 MiB | 34.42 MiB/s, done.\n",
            "Resolving deltas: 100% (220/220), done.\n",
            "Cloning into 'TRN-pytorch'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 263 (delta 0), reused 6 (delta 0), pack-reused 257\u001b[K\n",
            "Receiving objects: 100% (263/263), 69.11 KiB | 4.61 MiB/s, done.\n",
            "Resolving deltas: 100% (145/145), done.\n",
            "Submodule 'model_zoo' (https://github.com/yjxiong/tensorflow-model-zoo.torch) registered for path 'model_zoo'\n",
            "Cloning into '/content/TRN-pytorch/model_zoo'...\n",
            "remote: Enumerating objects: 621, done.        \n",
            "remote: Total 621 (delta 0), reused 0 (delta 0), pack-reused 621        \n",
            "Receiving objects: 100% (621/621), 43.66 MiB | 28.59 MiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n",
            "Submodule path 'model_zoo': checked out 'e31e0b7aa451e2c12c0107e616953a03d8cd0d47'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-hvqdCu5mp9",
        "colab_type": "text"
      },
      "source": [
        "## Import dei face embedding e dei metadati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F66e6c93YEdg",
        "colab_type": "code",
        "outputId": "8f5d09c3-373b-4a36-babc-037a5b93ec19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd /content/ProgettoSIIML/deeplearning/sports_recognition"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/ProgettoSIIML/deeplearning/sports_recognition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "botED2ixrScn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "known_faces = pd.read_csv('athletes_embedding.csv', sep=',',header=None).values\n",
        "athletes = pd.read_csv('athletes_metadata.csv', sep=',',header=None).values\n",
        "with open('sports1.csv') as file1:\n",
        "    reader1 = csv.reader(file1)\n",
        "    sports1 = {rows[0]:rows[1] for rows in reader1}\n",
        "with open('sports2.csv') as file2:\n",
        "    reader2 = csv.reader(file2)\n",
        "    sports2 = {rows[0]:rows[1] for rows in reader2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H49ucsAA6boS",
        "colab_type": "text"
      },
      "source": [
        "## Download del modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpIRzlbhrXYz",
        "colab_type": "code",
        "outputId": "3b87b226-f2f9-4dfd-da0d-436151adc512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd /content/TRN-pytorch/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TRN-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxBUbS5NuJI3",
        "colab_type": "code",
        "outputId": "bdddd3c7-ca94-4f48-f4ac-ccab614ebe14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        }
      },
      "source": [
        "% cd pretrain\n",
        "! ./download_models.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TRN-pytorch/pretrain\n",
            "Downloading TRNmultiscale on Something-Something\n",
            "--2019-04-14 15:48:32--  http://relation.csail.mit.edu/models/TRN_something_RGB_BNInception_TRNmultiscale_segment8_best.pth.tar\n",
            "Resolving relation.csail.mit.edu (relation.csail.mit.edu)... 128.30.100.255\n",
            "Connecting to relation.csail.mit.edu (relation.csail.mit.edu)|128.30.100.255|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 52711275 (50M) [application/x-tar]\n",
            "Saving to: ‘TRN_something_RGB_BNInception_TRNmultiscale_segment8_best.pth.tar’\n",
            "\n",
            "TRN_something_RGB_B 100%[===================>]  50.27M  68.9MB/s    in 0.7s    \n",
            "\n",
            "2019-04-14 15:48:33 (68.9 MB/s) - ‘TRN_something_RGB_BNInception_TRNmultiscale_segment8_best.pth.tar’ saved [52711275/52711275]\n",
            "\n",
            "--2019-04-14 15:48:33--  http://relation.csail.mit.edu/models/TRN_something_RGB_BNInception_TRN_segment3_best.pth.tar\n",
            "Resolving relation.csail.mit.edu (relation.csail.mit.edu)... 128.30.100.255\n",
            "Connecting to relation.csail.mit.edu (relation.csail.mit.edu)|128.30.100.255|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44205389 (42M) [application/x-tar]\n",
            "Saving to: ‘TRN_something_RGB_BNInception_TRN_segment3_best.pth.tar’\n",
            "\n",
            "TRN_something_RGB_B 100%[===================>]  42.16M  61.7MB/s    in 0.7s    \n",
            "\n",
            "2019-04-14 15:48:34 (61.7 MB/s) - ‘TRN_something_RGB_BNInception_TRN_segment3_best.pth.tar’ saved [44205389/44205389]\n",
            "\n",
            "--2019-04-14 15:48:34--  http://relation.csail.mit.edu/models/something_categories.txt\n",
            "Resolving relation.csail.mit.edu (relation.csail.mit.edu)... 128.30.100.255\n",
            "Connecting to relation.csail.mit.edu (relation.csail.mit.edu)|128.30.100.255|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7600 (7.4K) [text/plain]\n",
            "Saving to: ‘something_categories.txt’\n",
            "\n",
            "something_categorie 100%[===================>]   7.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-04-14 15:48:34 (616 MB/s) - ‘something_categories.txt’ saved [7600/7600]\n",
            "\n",
            "Downloading TRNmultiscale on Jester\n",
            "--2019-04-14 15:48:34--  http://relation.csail.mit.edu/models/TRN_jester_RGB_BNInception_TRNmultiscale_segment8_best.pth.tar\n",
            "Resolving relation.csail.mit.edu (relation.csail.mit.edu)... 128.30.100.255\n",
            "Connecting to relation.csail.mit.edu (relation.csail.mit.edu)|128.30.100.255|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 51653477 (49M) [application/x-tar]\n",
            "Saving to: ‘TRN_jester_RGB_BNInception_TRNmultiscale_segment8_best.pth.tar’\n",
            "\n",
            "TRN_jester_RGB_BNIn 100%[===================>]  49.26M  69.9MB/s    in 0.7s    \n",
            "\n",
            "2019-04-14 15:48:35 (69.9 MB/s) - ‘TRN_jester_RGB_BNInception_TRNmultiscale_segment8_best.pth.tar’ saved [51653477/51653477]\n",
            "\n",
            "--2019-04-14 15:48:35--  http://relation.csail.mit.edu/models/jester_categories.txt\n",
            "Resolving relation.csail.mit.edu (relation.csail.mit.edu)... 128.30.100.255\n",
            "Connecting to relation.csail.mit.edu (relation.csail.mit.edu)|128.30.100.255|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 527 [text/plain]\n",
            "Saving to: ‘jester_categories.txt’\n",
            "\n",
            "jester_categories.t 100%[===================>]     527  --.-KB/s    in 0s      \n",
            "\n",
            "2019-04-14 15:48:35 (73.9 MB/s) - ‘jester_categories.txt’ saved [527/527]\n",
            "\n",
            "Downloading TRNmultiscale on Moments in Time\n",
            "--2019-04-14 15:48:35--  http://relation.csail.mit.edu/models/TRN_moments_RGB_InceptionV3_TRNmultiscale_segment8_best.pth.tar\n",
            "Resolving relation.csail.mit.edu (relation.csail.mit.edu)... 128.30.100.255\n",
            "Connecting to relation.csail.mit.edu (relation.csail.mit.edu)|128.30.100.255|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 101160240 (96M) [application/x-tar]\n",
            "Saving to: ‘TRN_moments_RGB_InceptionV3_TRNmultiscale_segment8_best.pth.tar’\n",
            "\n",
            "TRN_moments_RGB_Inc 100%[===================>]  96.47M  83.2MB/s    in 1.2s    \n",
            "\n",
            "2019-04-14 15:48:36 (83.2 MB/s) - ‘TRN_moments_RGB_InceptionV3_TRNmultiscale_segment8_best.pth.tar’ saved [101160240/101160240]\n",
            "\n",
            "--2019-04-14 15:48:36--  http://relation.csail.mit.edu/models/moments_categories.txt\n",
            "Resolving relation.csail.mit.edu (relation.csail.mit.edu)... 128.30.100.255\n",
            "Connecting to relation.csail.mit.edu (relation.csail.mit.edu)|128.30.100.255|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3079 (3.0K) [text/plain]\n",
            "Saving to: ‘moments_categories.txt’\n",
            "\n",
            "moments_categories. 100%[===================>]   3.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-04-14 15:48:36 (235 MB/s) - ‘moments_categories.txt’ saved [3079/3079]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWRngOzQtPkb",
        "colab_type": "code",
        "outputId": "ef27f65c-2aff-417e-fca9-5d4dd8b752cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd /content/TRN-pytorch/model_zoo/bninception"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TRN-pytorch/model_zoo/bninception\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXoTskYWt3VD",
        "colab_type": "code",
        "outputId": "e6b2e6bd-bb25-4d9d-aeba-850a677fc621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile pytorch_load.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from .layer_factory import get_basic_layer, parse_expr\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import yaml\n",
        "\n",
        "\n",
        "class BNInception(nn.Module):\n",
        "    def __init__(self, model_path='model_zoo/bninception/bn_inception.yaml', num_classes=101,\n",
        "                       weight_url='https://yjxiong.blob.core.windows.net/models/bn_inception-9f5701afb96c8044.pth'):\n",
        "        super(BNInception, self).__init__()\n",
        "\n",
        "        manifest = yaml.load(open(model_path))\n",
        "\n",
        "        layers = manifest['layers']\n",
        "\n",
        "        self._channel_dict = dict()\n",
        "\n",
        "        self._op_list = list()\n",
        "        for l in layers:\n",
        "            out_var, op, in_var = parse_expr(l['expr'])\n",
        "            if op != 'Concat':\n",
        "                id, out_name, module, out_channel, in_name = get_basic_layer(l,\n",
        "                                                                3 if len(self._channel_dict) == 0 else self._channel_dict[in_var[0]],\n",
        "                                                                             conv_bias=True)\n",
        "\n",
        "                self._channel_dict[out_name] = out_channel\n",
        "                setattr(self, id, module)\n",
        "                self._op_list.append((id, op, out_name, in_name))\n",
        "            else:\n",
        "                self._op_list.append((id, op, out_var[0], in_var))\n",
        "                channel = sum([self._channel_dict[x] for x in in_var])\n",
        "                self._channel_dict[out_var[0]] = channel\n",
        "\n",
        "        #self.load_state_dict(torch.utils.model_zoo.load_url(weight_url))\n",
        "\n",
        "    def forward(self, input):\n",
        "        data_dict = dict()\n",
        "        data_dict[self._op_list[0][-1]] = input\n",
        "\n",
        "        def get_hook(name):\n",
        "\n",
        "            def hook(m, grad_in, grad_out):\n",
        "                print(name, grad_out[0].data.abs().mean())\n",
        "\n",
        "            return hook\n",
        "        for op in self._op_list:\n",
        "            if op[1] != 'Concat' and op[1] != 'InnerProduct':\n",
        "                data_dict[op[2]] = getattr(self, op[0])(data_dict[op[-1]])\n",
        "                # getattr(self, op[0]).register_backward_hook(get_hook(op[0]))\n",
        "            elif op[1] == 'InnerProduct':\n",
        "                x = data_dict[op[-1]]\n",
        "                data_dict[op[2]] = getattr(self, op[0])(x.view(x.size(0), -1))\n",
        "            else:\n",
        "                try:\n",
        "                    data_dict[op[2]] = torch.cat(tuple(data_dict[x] for x in op[-1]), 1)\n",
        "                except:\n",
        "                    for x in op[-1]:\n",
        "                        print(x,data_dict[x].size())\n",
        "                    raise\n",
        "        return data_dict[self._op_list[-1][2]]\n",
        "\n",
        "\n",
        "class InceptionV3(BNInception):\n",
        "    def __init__(self, model_path='model_zoo/bninception/inceptionv3.yaml', num_classes=101,\n",
        "                 weight_url='https://yjxiong.blob.core.windows.net/models/inceptionv3-cuhk-0e09b300b493bc74c.pth'):\n",
        "        super(InceptionV3, self).__init__(model_path=model_path, weight_url=weight_url, num_classes=num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting pytorch_load.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2i0TZbw6jvw",
        "colab_type": "text"
      },
      "source": [
        "## Import delle librerie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3wSCqUeu0q3",
        "colab_type": "code",
        "outputId": "6d6b5693-364e-4139-d9bc-7fdba7f9b161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/TRN-pytorch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/TRN-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yNcTfs4YiVs",
        "colab_type": "code",
        "outputId": "f52f54f1-d011-41b7-d90e-531716624c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "!pip install pafy\n",
        "!pip install youtube_dl\n",
        "!pip install face_recognition\n",
        "import cv2\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import pafy\n",
        "import face_recognition\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import json\n",
        "import re\n",
        "import argparse\n",
        "import functools\n",
        "import subprocess\n",
        "import moviepy.editor as mpy\n",
        "import ast\n",
        "import torchvision\n",
        "import torch.nn.parallel\n",
        "import torch.optim\n",
        "from models import TSN\n",
        "import transforms\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pafy\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/e8/3516f761558525b00d3eaf73744eed5c267db20650b7b660674547e3e506/pafy-0.5.4-py2.py3-none-any.whl\n",
            "Installing collected packages: pafy\n",
            "Successfully installed pafy-0.5.4\n",
            "Collecting youtube_dl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/d5/f3f7ff7fc4d05ba2b6d5904f21b43a625c3681e02fb620bcf789316d6664/youtube_dl-2019.4.7-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.8MB 16.6MB/s \n",
            "\u001b[?25hInstalling collected packages: youtube-dl\n",
            "Successfully installed youtube-dl-2019.4.7\n",
            "Collecting face_recognition\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/ed/ad9a28042f373d4633fc8b49109b623597d6f193d3bbbef7780a5ee8eef2/face_recognition-1.2.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from face_recognition) (1.16.2)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (19.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from face_recognition) (4.3.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.6/dist-packages (from face_recognition) (7.0)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 100.2MB 252kB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->face_recognition) (0.46)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.2.3 face-recognition-models-0.3.0\n",
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1892352/45929032 bytes (4.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5677056/45929032 bytes (12.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9412608/45929032 bytes (20.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13148160/45929032 bytes (28.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b16932864/45929032 bytes (36.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b20627456/45929032 bytes (44.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24174592/45929032 bytes (52.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27738112/45929032 bytes (60.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b31465472/45929032 bytes (68.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35176448/45929032 bytes (76.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b38895616/45929032 bytes (84.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b42573824/45929032 bytes (92.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZUnDnVg6pT9",
        "colab_type": "text"
      },
      "source": [
        "## Download del video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9efd4740-ec77-48ca-c0ff-9b5c1deef951",
        "id": "u33GIzQm_Ren",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "url = 'https://www.youtube.com/watch?v=v--6sCcpa6s'\n",
        "vPafy = pafy.new(url)\n",
        "play = vPafy.getbest()\n",
        "video = play.download(filepath=\"video.mp4\")\n",
        "print(vPafy.title)\n",
        "print(vPafy.keywords)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scotty James | Gold Medal | Men's Halfpipe | FIS Snowboard World Championships\n",
            "['Snowboard', 'Park & Pipe']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RqD8l6J6sWm",
        "colab_type": "text"
      },
      "source": [
        "## Riconoscimento degli atleti"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUXyzX_ZaTJw",
        "colab_type": "code",
        "outputId": "954b7fd1-2224-42c7-cd22-bb0f829e2036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2835
        }
      },
      "source": [
        "face_locations = []\n",
        "face_encodings = []\n",
        "frame_number = 0\n",
        "face_names = []\n",
        "\n",
        "input_movie = cv2.VideoCapture(video)\n",
        "length = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "print(\"Inizio processamento\")\n",
        "print(\"L'intero video contiene \" + str(length) + \" frame.\")\n",
        "\n",
        "while True:\n",
        "    \n",
        "    print(\"Stiamo analizzando il frame numero \" + str(frame_number))\n",
        "    # Grab a single frame of video\n",
        "    input_movie.set(1, frame_number)\n",
        "    ret, frame = input_movie.read()\n",
        "    frame_number += 20\n",
        "\n",
        "    # Quit when the input video file ends\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Find all the faces and face encodings in the current frame of video\n",
        "    face_locations = face_recognition.face_locations(frame)\n",
        "    face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
        "\n",
        "\n",
        "    for face_encoding in face_encodings:\n",
        "        # See if the face is a match for the known face(s)\n",
        "        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.42)\n",
        "        distance = face_recognition.face_distance(known_faces, face_encoding)\n",
        "        # name = None\n",
        "        temp_distance = 100\n",
        "        if True in match:\n",
        "            for value, count in zip(match, range(len(match))):\n",
        "                if value == True:\n",
        "                    if distance[count] < temp_distance:\n",
        "                        temp_distance = distance[count]\n",
        "                        name = athletes[count]\n",
        "            face_names.append(name[1])\n",
        "            print(\"è stata riconosciuta la faccia di \" + name[1])\n",
        "\n",
        "# All done!\n",
        "input_movie.release()\n",
        "cv2.destroyAllWindows()\n",
        "print(\"Fine\")\n",
        "print (face_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inizio processamento\n",
            "L'intero video contiene 8769 frame.\n",
            "Stiamo analizzando il frame numero 0\n",
            "Stiamo analizzando il frame numero 20\n",
            "Stiamo analizzando il frame numero 40\n",
            "Stiamo analizzando il frame numero 60\n",
            "Stiamo analizzando il frame numero 80\n",
            "Stiamo analizzando il frame numero 100\n",
            "Stiamo analizzando il frame numero 120\n",
            "Stiamo analizzando il frame numero 140\n",
            "Stiamo analizzando il frame numero 160\n",
            "Stiamo analizzando il frame numero 180\n",
            "Stiamo analizzando il frame numero 200\n",
            "Stiamo analizzando il frame numero 220\n",
            "Stiamo analizzando il frame numero 240\n",
            "Stiamo analizzando il frame numero 260\n",
            "Stiamo analizzando il frame numero 280\n",
            "Stiamo analizzando il frame numero 300\n",
            "Stiamo analizzando il frame numero 320\n",
            "Stiamo analizzando il frame numero 340\n",
            "Stiamo analizzando il frame numero 360\n",
            "Stiamo analizzando il frame numero 380\n",
            "Stiamo analizzando il frame numero 400\n",
            "Stiamo analizzando il frame numero 420\n",
            "Stiamo analizzando il frame numero 440\n",
            "Stiamo analizzando il frame numero 460\n",
            "Stiamo analizzando il frame numero 480\n",
            "Stiamo analizzando il frame numero 500\n",
            "Stiamo analizzando il frame numero 520\n",
            "Stiamo analizzando il frame numero 540\n",
            "Stiamo analizzando il frame numero 560\n",
            "Stiamo analizzando il frame numero 580\n",
            "Stiamo analizzando il frame numero 600\n",
            "Stiamo analizzando il frame numero 620\n",
            "Stiamo analizzando il frame numero 640\n",
            "Stiamo analizzando il frame numero 660\n",
            "Stiamo analizzando il frame numero 680\n",
            "Stiamo analizzando il frame numero 700\n",
            "Stiamo analizzando il frame numero 720\n",
            "Stiamo analizzando il frame numero 740\n",
            "Stiamo analizzando il frame numero 760\n",
            "Stiamo analizzando il frame numero 780\n",
            "Stiamo analizzando il frame numero 800\n",
            "Stiamo analizzando il frame numero 820\n",
            "Stiamo analizzando il frame numero 840\n",
            "Stiamo analizzando il frame numero 860\n",
            "Stiamo analizzando il frame numero 880\n",
            "Stiamo analizzando il frame numero 900\n",
            "Stiamo analizzando il frame numero 920\n",
            "Stiamo analizzando il frame numero 940\n",
            "Stiamo analizzando il frame numero 960\n",
            "Stiamo analizzando il frame numero 980\n",
            "Stiamo analizzando il frame numero 1000\n",
            "Stiamo analizzando il frame numero 1020\n",
            "Stiamo analizzando il frame numero 1040\n",
            "Stiamo analizzando il frame numero 1060\n",
            "Stiamo analizzando il frame numero 1080\n",
            "Stiamo analizzando il frame numero 1100\n",
            "Stiamo analizzando il frame numero 1120\n",
            "Stiamo analizzando il frame numero 1140\n",
            "Stiamo analizzando il frame numero 1160\n",
            "Stiamo analizzando il frame numero 1180\n",
            "è stata riconosciuta la faccia di sukanya srisurat\n",
            "Stiamo analizzando il frame numero 1200\n",
            "Stiamo analizzando il frame numero 1220\n",
            "è stata riconosciuta la faccia di long ma\n",
            "Stiamo analizzando il frame numero 1240\n",
            "Stiamo analizzando il frame numero 1260\n",
            "Stiamo analizzando il frame numero 1280\n",
            "Stiamo analizzando il frame numero 1300\n",
            "Stiamo analizzando il frame numero 1320\n",
            "Stiamo analizzando il frame numero 1340\n",
            "Stiamo analizzando il frame numero 1360\n",
            "Stiamo analizzando il frame numero 1380\n",
            "Stiamo analizzando il frame numero 1400\n",
            "Stiamo analizzando il frame numero 1420\n",
            "Stiamo analizzando il frame numero 1440\n",
            "Stiamo analizzando il frame numero 1460\n",
            "Stiamo analizzando il frame numero 1480\n",
            "Stiamo analizzando il frame numero 1500\n",
            "Stiamo analizzando il frame numero 1520\n",
            "Stiamo analizzando il frame numero 1540\n",
            "Stiamo analizzando il frame numero 1560\n",
            "Stiamo analizzando il frame numero 1580\n",
            "Stiamo analizzando il frame numero 1600\n",
            "Stiamo analizzando il frame numero 1620\n",
            "Stiamo analizzando il frame numero 1640\n",
            "Stiamo analizzando il frame numero 1660\n",
            "Stiamo analizzando il frame numero 1680\n",
            "Stiamo analizzando il frame numero 1700\n",
            "Stiamo analizzando il frame numero 1720\n",
            "è stata riconosciuta la faccia di yun chol om\n",
            "Stiamo analizzando il frame numero 1740\n",
            "Stiamo analizzando il frame numero 1760\n",
            "è stata riconosciuta la faccia di yun chol om\n",
            "Stiamo analizzando il frame numero 1780\n",
            "è stata riconosciuta la faccia di sukanya srisurat\n",
            "è stata riconosciuta la faccia di yuan cao\n",
            "Stiamo analizzando il frame numero 1800\n",
            "Stiamo analizzando il frame numero 1820\n",
            "Stiamo analizzando il frame numero 1840\n",
            "Stiamo analizzando il frame numero 1860\n",
            "Stiamo analizzando il frame numero 1880\n",
            "Stiamo analizzando il frame numero 1900\n",
            "Stiamo analizzando il frame numero 1920\n",
            "Stiamo analizzando il frame numero 1940\n",
            "Stiamo analizzando il frame numero 1960\n",
            "Stiamo analizzando il frame numero 1980\n",
            "Stiamo analizzando il frame numero 2000\n",
            "Stiamo analizzando il frame numero 2020\n",
            "Stiamo analizzando il frame numero 2040\n",
            "Stiamo analizzando il frame numero 2060\n",
            "è stata riconosciuta la faccia di jike zhang\n",
            "Stiamo analizzando il frame numero 2080\n",
            "Stiamo analizzando il frame numero 2100\n",
            "è stata riconosciuta la faccia di long ma\n",
            "Stiamo analizzando il frame numero 2120\n",
            "Stiamo analizzando il frame numero 2140\n",
            "è stata riconosciuta la faccia di yun chol om\n",
            "Stiamo analizzando il frame numero 2160\n",
            "Stiamo analizzando il frame numero 2180\n",
            "è stata riconosciuta la faccia di tingmao shi\n",
            "Stiamo analizzando il frame numero 2200\n",
            "Stiamo analizzando il frame numero 2220\n",
            "Stiamo analizzando il frame numero 2240\n",
            "Stiamo analizzando il frame numero 2260\n",
            "Stiamo analizzando il frame numero 2280\n",
            "Stiamo analizzando il frame numero 2300\n",
            "Stiamo analizzando il frame numero 2320\n",
            "Stiamo analizzando il frame numero 2340\n",
            "Stiamo analizzando il frame numero 2360\n",
            "Stiamo analizzando il frame numero 2380\n",
            "Stiamo analizzando il frame numero 2400\n",
            "Stiamo analizzando il frame numero 2420\n",
            "Stiamo analizzando il frame numero 2440\n",
            "Stiamo analizzando il frame numero 2460\n",
            "Stiamo analizzando il frame numero 2480\n",
            "Stiamo analizzando il frame numero 2500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-468de118a6ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Find all the faces and face encodings in the current frame of video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mface_locations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mface_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_locations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mface_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_trim_css_to_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rect_to_css\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_raw_face_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m_raw_face_locations\u001b[0;34m(img, number_of_times_to_upsample, model)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcnn_face_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_times_to_upsample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJk0shxpbvxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = Counter(face_names)\n",
        "output = [x[0] for x in c.items() if x[1] >= 2]\n",
        "print(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGZS_a-e67yF",
        "colab_type": "text"
      },
      "source": [
        "## Riconoscimento dell'attività(sport)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsi_GkHPpCMa",
        "colab_type": "code",
        "outputId": "fffc3dd8-1801-4e7d-fd6a-b48aeea584c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "def extract_frames(video_file, num_frames=8):\n",
        "    try:\n",
        "        os.makedirs(os.path.join(os.getcwd(), 'frames'))\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "    output = subprocess.Popen(['ffmpeg', '-i', video_file],\n",
        "                              stderr=subprocess.PIPE).communicate()\n",
        "    # Search and parse 'Duration: 00:05:24.13,' from ffmpeg stderr.\n",
        "    re_duration = re.compile('Duration: (.*?)\\.')\n",
        "    duration = re_duration.search(str(output[1])).groups()[0]\n",
        "    temp = [int(i) for i in duration.split(\":\")]\n",
        "\n",
        "    seconds = functools.reduce(lambda x, y: x * 60 + y,\n",
        "                               temp)\n",
        "    rate = num_frames / float(seconds)\n",
        "\n",
        "    output = subprocess.Popen(['ffmpeg', '-i', video_file,\n",
        "                               '-vf', 'fps={}'.format(rate),\n",
        "                               '-vframes', str(num_frames),\n",
        "                               '-loglevel', 'panic',\n",
        "                               'frames/%d.jpg']).communicate()\n",
        "    frame_paths = sorted([os.path.join('frames', frame)\n",
        "                          for frame in os.listdir('frames')])\n",
        "\n",
        "    frames = load_frames(frame_paths)\n",
        "    subprocess.call(['rm', '-rf', 'frames'])\n",
        "    return frames\n",
        "\n",
        "\n",
        "def load_frames(frame_paths, num_frames=8):\n",
        "    frames = [Image.open(frame).convert('RGB') for frame in frame_paths]\n",
        "    if len(frames) >= num_frames:\n",
        "        return frames[::int(np.ceil(len(frames) / float(num_frames)))]\n",
        "    else:\n",
        "        raise ValueError('Video must have at least {} frames'.format(num_frames))\n",
        "\n",
        "\n",
        "def render_frames(frames, prediction):\n",
        "    rendered_frames = []\n",
        "    for frame in frames:\n",
        "        img = np.array(frame)\n",
        "        height, width, _ = img.shape\n",
        "        cv2.putText(img, prediction,\n",
        "                    (1, int(height / 8)),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    1, (255, 255, 255), 2)\n",
        "        rendered_frames.append(img)\n",
        "    return rendered_frames\n",
        "\n",
        "\n",
        "# Get dataset categories.\n",
        "categories_file = 'pretrain/moments_categories.txt'\n",
        "categories = [line.rstrip() for line in open(categories_file, 'r').readlines()]\n",
        "num_class = len(categories)\n",
        "\n",
        "arch = 'InceptionV3'\n",
        "\n",
        "# Load model.\n",
        "net = TSN(num_class,\n",
        "          8,\n",
        "          'RGB',\n",
        "          base_model= arch,\n",
        "          consensus_type='TRNmultiscale',\n",
        "          img_feature_dim=256, print_spec=False)\n",
        "\n",
        "checkpoint = torch.load('pretrain/TRN_moments_RGB_InceptionV3_TRNmultiscale_segment8_best.pth.tar')\n",
        "base_dict = {'.'.join(k.split('.')[1:]): v for k, v in list(checkpoint['state_dict'].items())}\n",
        "net.load_state_dict(base_dict)\n",
        "net.cuda().eval()\n",
        "\n",
        "# Initialize frame transforms.\n",
        "transform = torchvision.transforms.Compose([\n",
        "    transforms.GroupOverSample(net.input_size, net.scale_size),\n",
        "    transforms.Stack(roll=(arch in ['BNInception', 'InceptionV3'])),\n",
        "    transforms.ToTorchFormatTensor(div=(arch not in ['BNInception', 'InceptionV3'])),\n",
        "    transforms.GroupNormalize(net.input_mean, net.input_std),\n",
        "])\n",
        "\n",
        "# Obtain video frames\n",
        "print('Extracting frames using ffmpeg...')\n",
        "frames = extract_frames(\"video.mp4\", 8)\n",
        "\n",
        "\n",
        "# Make video prediction.\n",
        "data = transform(frames)\n",
        "input = data.view(-1, 3, data.size(1), data.size(2)).unsqueeze(0).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = net(input)\n",
        "    h_x = torch.mean(F.softmax(logits, 1), dim=0).data\n",
        "    probs, idx = h_x.sort(0, True)\n",
        "\n",
        "# Output the prediction.\n",
        "video_name = \"video\"\n",
        "predictions = []\n",
        "print('RESULT ON ' + video_name)\n",
        "for i in range(0, 3):\n",
        "    print('{:.3f} -> {}'.format(probs[i], categories[idx[i]]))\n",
        "    predictions.append(categories[idx[i]])\n",
        "    \n",
        "def get_result(results):\n",
        "  for result in results:\n",
        "    if sports1.get(result) != None:\n",
        "      return sports1.get(result)\n",
        "  for result in results:\n",
        "    if sports2.get(result) != None:\n",
        "      return sports2.get(result)\n",
        "if get_result(predictions) != None:\n",
        "  print(\"Sport recognized is \" + ast.literal_eval(get_result(predictions))[0])\n",
        "  output = output + ast.literal_eval(get_result(predictions))\n",
        "else:\n",
        "  print(\"No sport recognized\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/content/TRN-pytorch/models.py:87: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
            "  normal(self.new_fc.weight, 0, std)\n",
            "\n",
            "WARNING:py.warnings:/content/TRN-pytorch/models.py:88: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "  constant(self.new_fc.bias, 0)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Multi-Scale Temporal Relation Network Module in use ['8-frame relation', '7-frame relation', '6-frame relation', '5-frame relation', '4-frame relation', '3-frame relation', '2-frame relation']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:208: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Freezing BatchNorm2D except the first one.\n",
            "Extracting frames using ffmpeg...\n",
            "RESULT ON video\n",
            "0.732 -> skiing\n",
            "0.035 -> skating\n",
            "0.029 -> racing\n",
            "No sport recognized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCLDWle_JNK1",
        "colab_type": "text"
      },
      "source": [
        "## Input per PredictionIO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn_egWDM7BPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = [x.lower() for x in output]\n",
        "print(\"Input for recommender system is: \" + str(output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSG4kXUQaA-o",
        "colab_type": "code",
        "outputId": "ad7046bd-4c57-424c-8304-cd39e0c8f63f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Apr 15 16:23:14 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    71W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIOpZU3caBcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! conda list cudnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjt5FxpqgBNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.platform import build_info as tf_build_info\n",
        "print(tf_build_info.cuda_version_number)\n",
        "# 9.0 in v1.10.0\n",
        "print(tf_build_info.cudnn_version_number)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4HUc_X5gPDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}